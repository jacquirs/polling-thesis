---
title: "Gov 1347 Replication"
author: "Jacqui Schlesinger"
date: "2026-02-06"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```


```{r, results = 'hide'}
# load libraries
library(car)
library(caret)
library(CVXR)
library(glmnet)
library(tidyverse)
library(knitr)
library(kableExtra)
library(plotly)
library(lubridate)
library(scales)
library(ggplot2)
library(dplyr)
library(htmlwidgets)
```


```{r}
####----------------------------------------------------------#
#### Read, merge, and process data
####----------------------------------------------------------#

# FiveThirtyEight polling average datasets national
d_pollav_natl <- read_csv("data/gov1347_national_polls_1968-2024partial.csv")

# Fiftypluysone polling average dataset
d_fiftyplusone <- read_csv("data/president_2024_general.csv")

# pollster rating from 538
d_pollster_ratings <- read_csv("data/gov1347_pollster-ratings-combined.csv")

# polling data from 2020
d_poll_2020 <- read_csv("data/gov1347_president_polls_2020.csv")

# polling data from 2016
d_poll_2016 <- read_csv("data/gov1347_president_polls_2016.csv") 
# popular vote data through 2020
d_vote <- read_csv("data/gov1347_popvote_1948-2020.csv")
d_vote$party[d_vote$party == "democrat"] <- "DEM"
d_vote$party[d_vote$party == "republican"] <- "REP"
```


### Recent Polling Trends and the Use of Polling in Models
Polling-- originating from Francis Galton’s 1907 [Vox populi](https://www.nature.com/articles/075450a0.pdf) on the reliability of popular judgement-- aims to gauge public sentiment but faces challenges in predicting crystallized outcomes. Thousands of polls are conducted to understand the race at a given time. But with so many complications in measuring the behavior of voters, it is extremely difficult for a poll to get the "right" answer.

These difficulties of polling are highlighted in Gelman and King's ["Why are american presidential election campaign polls so variable when votes are so predictable?"](https://www-jstor-org.ezp-prod1.hul.harvard.edu/stable/194212?sid=primo) Variability in polls, as discussed in the analysis, shows that while polls fluctuate due to unforeseen events, overall outcomes remain largely predictable based on fundamental factors. While voters may appear to change their minds during the campaign, they often make final decisions based on rational, long-standing preferences. Temporary polling swings often don’t reflect lasting changes in voter intentions.

For an example of this, look at variations in the 2020 election polling, with "game-changers" indicating significant events that may have influenced polling averages temporarily.


```{r}
####----------------------------------------------------------#
#### Year by year polling over time, test with 2020
####----------------------------------------------------------#

polling_over_time_2020 <- d_pollav_natl |> 
  filter(year == 2020) |> 
  ggplot(aes(x = poll_date, y = poll_support, color = party, group = party)) +
  geom_line(linewidth = 1) +
  geom_point(aes(text = paste("Date:", poll_date, "<br>Avg Support:", round(poll_support,3), "%"))) + 
  geom_point(size = 1.8, alpha = 0.7) +
  scale_x_date(
    breaks = scales::date_breaks("1 month"),
    date_labels = "%b",
    expand = expansion(mult = c(0.01, 0.05))
  ) +
  scale_y_continuous(labels = scales::label_percent(scale = 1)) +
  scale_color_manual(values = c("dodgerblue4", "firebrick1")) +
  labs(
    x = NULL,
    y = "Average poll approval",
    title = "Polling Averages by Date, 2020",
    subtitle = "From 538 Data",
    color = NULL
  ) +
  theme_classic(base_size = 12) +
  theme(
    plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5, size = 12),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    legend.position = "top",
    panel.grid.major.y = element_line(color = "grey85"),
    panel.grid.major.x = element_blank(),
    panel.grid.minor = element_blank()
  )

polling_plot_2020_static

# convert to interactive plotly
interactive_polling_over_time_2020 <- ggplotly(polling_over_time_2020, tooltip = "text")
interactive_polling_over_time_2020

# save as a flot plot
ggsave(
  filename = "figures/fiftyplusoneR/polling_over_time_2020.png",
  plot = polling_over_time_2020,
  width = 8,
  height = 5
)

```


```{r}
out_dir <- "figures/fiftyplusoneR"

# repeats 2020 process for 2000-2016
make_polling_plot <- function(data, yr) {
  p <- data %>%
    filter(year == yr) %>%
    ggplot(aes(x = poll_date, y = poll_support, color = party, group = party)) +
    geom_line(linewidth = 1) +
    geom_point(aes(
      text = paste("Date:", poll_date,
                   "<br>Avg Support:", round(poll_support, 3), "%")
    ),
    size = 1.8, alpha = 0.7) +
    scale_x_date(
      breaks = scales::date_breaks("1 month"),
      date_labels = "%b",
      expand = expansion(mult = c(0.01, 0.05))
    ) +
    scale_y_continuous(labels = scales::label_percent(scale = 1)) +
    scale_color_manual(values = c("dodgerblue4", "firebrick1")) +
    labs(
      x = NULL,
      y = "Average poll approval",
      title = paste("Polling Averages by Date,", yr),
      subtitle = "From 538 Data",
      color = NULL
    ) +
    theme_classic(base_size = 12) +
    theme(
      plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
      plot.subtitle = element_text(hjust = 0.5, size = 12),
      axis.text.x = element_text(size = 10),
      axis.text.y = element_text(size = 10),
      legend.position = "top",
      panel.grid.major.y = element_line(color = "grey85"),
      panel.grid.major.x = element_blank(),
      panel.grid.minor = element_blank()
    )

  interactive <- ggplotly(p, tooltip = "text")

  list(
    static = p,
    interactive = interactive
  )
}

# years to render
years <- seq(2000, 2016, by = 4)

# generate plots and save
polling_plots <- list()

for (y in years) {
  plots <- make_polling_plot(d_pollav_natl, y)
  polling_plots[[as.character(y)]] <- plots

  # save PNG 
  static_fname_png <- file.path(out_dir, paste0("polling_over_time_", y, ".png"))

  ggsave(
    filename = static_fname_png,
    plot = plots$static,
    width = 8,
    height = 5,
    dpi = 300
  )

  # save interactive HTML
  interactive_fname <- file.path(out_dir, paste0("polling_over_time_", y, ".html"))
  # convert to html widget
  saveWidget(plots$interactive, file = interactive_fname, selfcontained = TRUE)
}

```


Whether Biden's polling data can be used without adjustment as a proxy for Harris like here is an open question.

### Pollster Quality Evaluation
Different pollster methodologies and decisions impact the accuracy of polls. To account for this, FiveThirtyEight creates pollster ratings called "pollscores" which account for bias and error, and also look at transparency and percent partisan work. Variation in pollster quality can provide valuable information about the quality of the polling data my model relies on.

```{r}
####----------------------------------------------------------#
#### Understand what's in the dataset of pollster ratings
####----------------------------------------------------------#

data_dict <- tibble(
  column = names(d_pollster_ratings),
  class = sapply(d_pollster_ratings, \(x) paste(class(x), collapse = ", ")),
  n_missing = sapply(d_pollster_ratings, \(x) sum(is.na(x))),
  pct_missing = round(100 * n_missing / nrow(d_pollster_ratings), 1),
  n_unique = sapply(d_pollster_ratings, \(x) dplyr::n_distinct(x, na.rm = TRUE)),
  example_values = sapply(d_pollster_ratings, \(x) {
    vals <- unique(x[!is.na(x)])
    vals <- head(vals, 3)
    paste(vals, collapse = ", ")
  })
) %>% arrange(desc(pct_missing))

kable(data_dict) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))

```

```{r}
####----------------------------------------------------------#
#### Table of mean, median ,sd, and percentiels of all numeric cols
####----------------------------------------------------------#

num_summary <- d_pollster_ratings %>%
  summarise(across(where(is.numeric),
                   list(
                     n = ~sum(!is.na(.)),
                     mean = ~mean(., na.rm = TRUE),
                     sd = ~sd(., na.rm = TRUE),
                     min = ~min(., na.rm = TRUE),
                     p25 = ~quantile(., 0.25, na.rm = TRUE),
                     median = ~median(., na.rm = TRUE),
                     p75 = ~quantile(., 0.75, na.rm = TRUE),
                     max = ~max(., na.rm = TRUE)
                   ),
                   .names = "{.col}__{.fn}")) %>%
  pivot_longer(everything(),
               names_to = c("Variable", ".value"),
               names_sep = "__")

kable(num_summary, digits = 3) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))

```

```{r}
####----------------------------------------------------------#
#### Basic histrograms to understand values
####----------------------------------------------------------#
ggplot(d_pollster_ratings, aes(bias_ppm)) + geom_histogram(bins = 30)

ggplot(d_pollster_ratings, aes(error_ppm)) + geom_histogram(bins = 30)

ggplot(d_pollster_ratings, aes(POLLSCORE)) + geom_histogram(bins = 30)

```


```{r}
####----------------------------------------------------------#
#### Pollster rating variance calcs
####----------------------------------------------------------#

# mean and 95% CI functions
mean_ci <- function(x, conf = 0.95) {
  x <- x[!is.na(x)]
  n <- length(x)
  mean_x <- mean(x)
  se <- sd(x) / sqrt(n)
  alpha <- 1 - conf
  tcrit <- qt(1 - alpha/2, df = n - 1)
  moe <- tcrit * se
  c(mean = mean_x, lower = mean_x - moe, upper = mean_x + moe, n = n)
}

# get CI for variety of variables on pollsters
bias_ci <- mean_ci(d_pollster_ratings$bias_ppm)
error_ci <- mean_ci(d_pollster_ratings$error_ppm)
pollscore_ci <- mean_ci(d_pollster_ratings$POLLSCORE)
numeric_grade_ci <- mean_ci(d_pollster_ratings$numeric_grade)
wtd_avg_transparency_ci <- mean_ci(d_pollster_ratings$wtd_avg_transparency)
percent_partisan_work_ci <- mean_ci(d_pollster_ratings$percent_partisan_work)

# get min and max values for comparison column (ended up not using)
bias_min <- min(d_pollster_ratings$bias_ppm, na.rm = TRUE)
error_min <- min(d_pollster_ratings$error_ppm, na.rm = TRUE)
pollscore_min <- min(d_pollster_ratings$POLLSCORE, na.rm = TRUE)
partisan_work_min <- min(d_pollster_ratings$percent_partisan_work, na.rm = TRUE)
numeric_grade_max <- max(d_pollster_ratings$numeric_grade, na.rm = TRUE)
wtd_avg_transparency_max <- max(d_pollster_ratings$wtd_avg_transparency, na.rm = TRUE)

# put together for table
ci_data <- data.frame(
  Metric = c("Bias PPM", "Error PPM", "Pollscore", "Numeric Grade", "Weighted Avg Transparency", "Percent Partisan Work"),
  
  Mean = c(round(bias_ci["mean"], 3), round(error_ci["mean"], 3), round(pollscore_ci["mean"], 3), paste0(round(numeric_grade_ci["mean"], 3), "/3"), paste0(round(wtd_avg_transparency_ci["mean"], 3), "/10"), paste0(round(percent_partisan_work_ci["mean"] * 100, 2), "%")),
  
  Lower_CI = c(round(bias_ci["lower"], 3), round(error_ci["lower"], 3), round(pollscore_ci["lower"], 3), paste0(round(numeric_grade_ci["lower"], 3), "/3"), paste0(round(wtd_avg_transparency_ci["lower"], 3), "/10"), paste0(round(percent_partisan_work_ci["lower"] * 100, 2), "%")),
  
  Upper_CI = c(round(bias_ci["upper"], 3), round(error_ci["upper"], 3), round(pollscore_ci["upper"], 3), paste0(round(numeric_grade_ci["upper"], 3), "/3"), paste0(round(wtd_avg_transparency_ci["upper"], 3), "/10"), paste0(round(percent_partisan_work_ci["upper"] * 100, 2), "%"))
)

# table styling
kable(ci_data) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

These summary statistics for pollster quality reveal the limits of polling data. The current number one ranked pollster (The New York Times/Siena College) has a pollscore of -1.5, the best possible, no partisan work, and 8.7/10 transparency score. It is worth noting that the Predictive Plus-Minus for pollster's absolute error, or error_ppm, is calculated as 

$$predictive\ error = adjusted\ error * (n / (n + n_{shrinkage})) + (group\ error\ prior) * (n_{shrinkage} / (n + n_{shrinkage}))$$

, and	Predictive Plus-Minus for pollster bias, or bias_ppm, is calculated as 

$$predictive\ bias = adjusted\ bias * (n / (n + n_{shrinkage})) + (group\ error\ prior) * (n_{shrinkage} / (n + n_{shrinkage}))$$

, per [538 methodology](https://abcnews.go.com/538/538s-pollster-ratings-work/story?id=105398138), where n is the time-weighted number of polls the pollster has released and n_shrinkage is an integer that represents the effective number of polls' worth of weight to put on the prior.

Through these complicated formulas, it is clear that pollster ratings are not particularly inspiring in accuracy of their poll results, at least according to these ratings. It is worth noting that transparency, which is a part of 538's model, is not necessarily a direct correlate with accuracy of results, though this is difficult to test on a poll by poll basis given the lack of true comparison values.

